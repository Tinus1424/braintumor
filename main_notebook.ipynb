{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from main import *\n",
    "from hyperparameter import *\n",
    "from transferlearning import *\n",
    "\n",
    "#Set to False to also run hyperparameter tuning and transfer learning\n",
    "JUSTBASELINE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the paths based on your Drive directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "data_dir ='data' # Change to the path to your directory\n",
    "train_images_dir = os.path.join(data_dir, 'Training')\n",
    "test_images_dir = os.path.join(data_dir, 'Testing')\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Training directory: {train_images_dir}\")\n",
    "print(f\"Testing directory: {test_images_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't touch this block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the training and test data\n",
    "try:\n",
    "    train_data, test_data = get_images(train_images_dir, test_images_dir)\n",
    "    print(\"Data loading completed successfully.\")\n",
    "    print(f\"Number of training samples: {train_data.samples}\")\n",
    "    print(f\"Number of test samples: {test_data.samples}\")\n",
    "    print(f\"Class names: {train_data.class_indices}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing 15 random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_batch(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distribution(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, validation, and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = train_val_split(train_data)\n",
    "X_test, y_test = test_splits(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_train = list(train_data.class_indices.keys())\n",
    "class_names_test = list(test_data.class_indices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = baseline()\n",
    "history = base.fit(X_train, y_train, batch_size =32, epochs = 10, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [base]\n",
    "metr, f1 = get_metrics(models, X_test, y_test, class_names_test)\n",
    "display(metr, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_metric(history, \"accuracy\")\n",
    "summarize_metric(history, \"loss\")\n",
    "\n",
    "roc_auc_val = plot_roc_curve(base, X_val, y_val, class_names_train,title =\"Validation Set\")\n",
    "plot_confusion_matrix(base, X_val, y_val, class_names_train, normalize=False,title =\"Validation Set\")\n",
    "\n",
    "roc_auc_val = plot_roc_curve(base, X_test, y_test, class_names_test, title = \"Test Set\")\n",
    "plot_confusion_matrix(base, X_test, y_test, class_names_test, normalize=False, title = \"Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning\n",
    "dense units, filters, kernels, max pooling, activation, LR are all tuneable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if JUSTBASELINE:\n",
    "    print(\"Set JUSTBASELINE to False to run the following blocks\")\n",
    "    %\n",
    "\n",
    "# change this project name for every run \n",
    "project_name = \"baseline_hyperparameter_tuning1\"\n",
    "tuner_result = tune_hyperparameters(model_hyperparameter_tuning, project_name)\n",
    "print_tuning_summary(tuner_result, project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "import transferlearning as tfl\n",
    "resolution=32\n",
    "\n",
    "vgg16m_model = tfl.transfer_learning(VGG16, resolution,X_train, y_train, X_val, y_val, epochs=20, batch_size=32, optimizer = \"adam\")\n",
    "\n",
    "roc_auc_val = plot_roc_curve(vgg16m_model, X_val, y_val, class_names_train,title =\"Validation Set\")\n",
    "plot_confusion_matrix(vgg16m_model, X_val, y_val, class_names_train, normalize=False,title =\"Validation Set\")\n",
    "\n",
    "roc_auc_val = plot_roc_curve(vgg16m_model, X_test, y_test, class_names_test, title = \"Test Set\")\n",
    "plot_confusion_matrix(vgg16m_model, X_test, y_test, class_names_test, normalize=False, title = \"Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "resnet50_model = transfer_learning(ResNet50, resolution,X_train, y_train, X_val, y_val, epochs=10, batch_size=32, optimizer = \"adam\")\n",
    "\n",
    "roc_auc_val = plot_roc_curve(resnet50_model, X_val, y_val, class_names_train,title =\"Validation Set\")\n",
    "plot_confusion_matrix(resnet50_model, X_val, y_val, class_names_train, normalize=False,title =\"Validation Set\")\n",
    "\n",
    "roc_auc_val = plot_roc_curve(resnet50_model, X_test, y_test, class_names_test, title = \"Test Set\")\n",
    "plot_confusion_matrix(resnet50_model, X_test, y_test, class_names_test, normalize=False, title = \"Test Set\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Densenet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from tensorflow.keras.applications import DenseNet121\n",
    "\n",
    "densenet121_model = transfer_learning(DenseNet121, resolution,X_train, y_train, X_val, y_val, epochs=10, batch_size=32, optimizer = \"adam\")\n",
    "\n",
    "roc_auc_val = plot_roc_curve(densenet121_model, X_val, y_val, class_names_train,title =\"Validation Set\")\n",
    "plot_confusion_matrix(densenet121_model, X_val, y_val, class_names_train, normalize=False,title =\"Validation Set\")\n",
    "\n",
    "roc_auc_val = plot_roc_curve(densenet121_model, X_test, y_test, class_names_test, title = \"Test Set\")\n",
    "plot_confusion_matrix(densenet121_model, X_test, y_test, class_names_test, normalize=False, title = \"Test Set\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a model suggested in a medium article for mnist that I tried for our dataset and it works quite well: https://medium.com/@BrendanArtley/mnist-keras-simple-cnn-99-6-731b624aee7f. Getting 84%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Flatten, Dropout, BatchNormalization, Activation, Input, GlobalAveragePooling2D, LSTM, Embedding, GRU, Conv1D, Conv3D\n",
    "def model_mnist():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same',\n",
    "                     input_shape=(30,30,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid' ))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', strides=1, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='valid', strides=2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    optimizer = Adam(0.001, beta_1=0.9, beta_2=0.999 )\n",
    "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "mnist_model = model_mnist()\n",
    "reduce_lr = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n",
    "history = mnist_model.fit(X_train, y_train, batch_size =32, epochs = 30, validation_data = (X_val, y_val), callbacks = [reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pim's model. Getting 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential,  layers, Input, optimizers\n",
    "\n",
    "def hyperparam(): \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape = (30, 30, 1)))\n",
    "    \n",
    "    model.add(layers.Conv2D(128, (3, 3), activation=\"relu\", padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation=\"relu\", padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2), strides=2))\n",
    "    \n",
    "    model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(16, (3, 3), activation=\"relu\", padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256, activation= 'relu'))\n",
    "    model.add(layers.Dropout(rate = 0.5))\n",
    "    model.add(layers.Dense(32, activation = \"relu\"))\n",
    "    model.add(layers.Dropout(rate = 0.25))\n",
    "    model.add(layers.Dense(16, activation = 'relu'))\n",
    "    model.add(layers.Dense(4, activation = \"softmax\"))\n",
    "\n",
    "    model.compile(optimizer = optimizers.Adam(learning_rate= 0.001),\n",
    "                    loss = \"categorical_crossentropy\",\n",
    "                    metrics = [\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = hyperparam()\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, batch_size =32, epochs = 30, validation_data = (X_val, y_val), verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential,  layers, Input, optimizers\n",
    "\n",
    "def lenet5(): \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape = (30, 30, 1)))\n",
    "    \n",
    "    model.add(layers.Conv2D(6, (5, 5), strides=1, activation=\"tanh\", padding='same'))\n",
    "    model.add(layers.AveragePooling2D(pool_size=(2, 2), strides=2, padding='valid'))\n",
    "    \n",
    "    model.add(layers.Conv2D(16, (5, 5), strides=1, activation=\"tanh\", padding='valid'))\n",
    "    model.add(layers.AveragePooling2D(pool_size=(2, 2), strides=2, padding='valid'))\n",
    "    \n",
    "    model.add(layers.Conv2D(filters=120, kernel_size=(5, 5), strides=1, padding='valid', activation='tanh'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(layers.Dense(units=84, activation='tanh'))\n",
    "    \n",
    "    model.add(layers.Dense(units=4, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer = optimizers.Adam(learning_rate= 0.001),\n",
    "                    loss = \"categorical_crossentropy\",\n",
    "                    metrics = [\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = lenet5()\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, batch_size =16, epochs = 30, validation_data = (X_val, y_val), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
