{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8421666-b093-41f5-be4e-91ed798a3e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 10:06:15.129316: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-26 10:06:15.143654: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-26 10:06:15.157987: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-26 10:06:15.162394: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-26 10:06:15.175909: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-26 10:06:16.166370: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: data\n",
      "Training directory: data/Training\n",
      "Testing directory: data/Testing\n"
     ]
    }
   ],
   "source": [
    "from main import * \n",
    "\n",
    "data_dir ='data' # Change to the path to your directory\n",
    "train_images_dir = os.path.join(data_dir, 'Training')\n",
    "test_images_dir = os.path.join(data_dir, 'Testing')\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Training directory: {train_images_dir}\")\n",
    "print(f\"Testing directory: {test_images_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "617c5b78-df38-44a0-b1a4-ba7aeae75bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5712 images belonging to 4 classes.\n",
      "Found 1311 images belonging to 4 classes.\n",
      "Data loading completed successfully.\n",
      "Number of training samples: 5712\n",
      "Number of test samples: 1311\n",
      "Class names: {'glioma': 0, 'meningioma': 1, 'notumor': 2, 'pituitary': 3}\n"
     ]
    }
   ],
   "source": [
    "# Load the training and test data\n",
    "try:\n",
    "    train_data, test_data = get_images(train_images_dir, test_images_dir)\n",
    "    print(\"Data loading completed successfully.\")\n",
    "    print(f\"Number of training samples: {train_data.samples}\")\n",
    "    print(f\"Number of test samples: {test_data.samples}\")\n",
    "    print(f\"Class names: {train_data.class_indices}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95fc163a-79de-4c72-ae7f-1dc0e35bb2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in the training data: 179\n",
      "Batch size of a single batch 32\n",
      "Number of samples in the training dataset 5712\n",
      "\n",
      "Number of training data batches with val split of 0.2: 144\n",
      "Number of validation data batches: 35\n",
      "\n",
      "Shape of image training set: (4608, 30, 30, 1)\n",
      "Shape of image validation set: (1104, 30, 30, 1)\n",
      "\n",
      "Shape of label training set: (4608, 4)\n",
      "Shape of label validation set: (1104, 4)\n",
      "Number of batches in the test data: 41\n",
      "Batch size of a single batch 32\n",
      "Number of samples in the test dataset 1311\n",
      "\n",
      "Shape of image test set: (1311, 30, 30, 1)\n",
      "\n",
      "Shape of label test set: (1311, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = train_val_split(train_data)\n",
    "X_test, y_test = test_splits(test_data)\n",
    "class_names_train = list(train_data.class_indices.keys())\n",
    "class_names_test = list(test_data.class_indices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9a7329f-b0b4-435b-8ee8-c88e500aa34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m143/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3185 - loss: 1.4916"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1727338271.743631    4455 assert_op.cc:38] Ignoring Assert operator sequential_5_1/lambda_16_1/grayscale_to_rgb/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1727338271.744841    4455 assert_op.cc:38] Ignoring Assert operator sequential_5_1/lambda_16_1/grayscale_to_rgb/assert_greater_equal/Assert/Assert\n",
      "W0000 00:00:1727338273.855314    4454 assert_op.cc:38] Ignoring Assert operator sequential_5_1/lambda_16_1/grayscale_to_rgb/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1727338273.855431    4454 assert_op.cc:38] Ignoring Assert operator sequential_5_1/lambda_16_1/grayscale_to_rgb/assert_greater_equal/Assert/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 59ms/step - accuracy: 0.3193 - loss: 1.4897 - val_accuracy: 0.3732 - val_loss: 1.2741\n",
      "Epoch 2/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.4721 - loss: 1.2053 - val_accuracy: 0.4728 - val_loss: 1.1620\n",
      "Epoch 3/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.5315 - loss: 1.1118 - val_accuracy: 0.5118 - val_loss: 1.1108\n",
      "Epoch 4/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.5637 - loss: 1.0525 - val_accuracy: 0.5399 - val_loss: 1.0785\n",
      "Epoch 5/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.5840 - loss: 1.0093 - val_accuracy: 0.5462 - val_loss: 1.0556\n",
      "Epoch 6/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.6017 - loss: 0.9756 - val_accuracy: 0.5571 - val_loss: 1.0379\n",
      "Epoch 7/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.6142 - loss: 0.9481 - val_accuracy: 0.5652 - val_loss: 1.0236\n",
      "Epoch 8/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.6223 - loss: 0.9251 - val_accuracy: 0.5734 - val_loss: 1.0110\n",
      "Epoch 9/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.6313 - loss: 0.9053 - val_accuracy: 0.5806 - val_loss: 1.0002\n",
      "Epoch 10/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.6373 - loss: 0.8882 - val_accuracy: 0.5870 - val_loss: 0.9899\n",
      "Epoch 11/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.6423 - loss: 0.8731 - val_accuracy: 0.5888 - val_loss: 0.9805\n",
      "Epoch 12/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.6511 - loss: 0.8596 - val_accuracy: 0.5906 - val_loss: 0.9717\n",
      "Epoch 13/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.6558 - loss: 0.8475 - val_accuracy: 0.5906 - val_loss: 0.9634\n",
      "Epoch 14/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.6619 - loss: 0.8364 - val_accuracy: 0.5942 - val_loss: 0.9555\n",
      "Epoch 15/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.6656 - loss: 0.8262 - val_accuracy: 0.6051 - val_loss: 0.9481\n",
      "Epoch 16/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.6698 - loss: 0.8168 - val_accuracy: 0.6096 - val_loss: 0.9411\n",
      "Epoch 17/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.6737 - loss: 0.8080 - val_accuracy: 0.6196 - val_loss: 0.9346\n",
      "Epoch 18/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.6760 - loss: 0.7999 - val_accuracy: 0.6223 - val_loss: 0.9282\n",
      "Epoch 19/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.6778 - loss: 0.7923 - val_accuracy: 0.6232 - val_loss: 0.9222\n",
      "Epoch 20/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.6802 - loss: 0.7850 - val_accuracy: 0.6250 - val_loss: 0.9163\n",
      "Epoch 21/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.6841 - loss: 0.7781 - val_accuracy: 0.6295 - val_loss: 0.9108\n",
      "Epoch 22/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.6866 - loss: 0.7715 - val_accuracy: 0.6359 - val_loss: 0.9053\n",
      "Epoch 23/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.6912 - loss: 0.7652 - val_accuracy: 0.6404 - val_loss: 0.8999\n",
      "Epoch 24/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.6935 - loss: 0.7592 - val_accuracy: 0.6458 - val_loss: 0.8948\n",
      "Epoch 25/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.6962 - loss: 0.7533 - val_accuracy: 0.6495 - val_loss: 0.8899\n",
      "Epoch 26/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.7011 - loss: 0.7477 - val_accuracy: 0.6522 - val_loss: 0.8853\n",
      "Epoch 27/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.7039 - loss: 0.7423 - val_accuracy: 0.6522 - val_loss: 0.8811\n",
      "Epoch 28/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.7063 - loss: 0.7372 - val_accuracy: 0.6567 - val_loss: 0.8767\n",
      "Epoch 29/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.7083 - loss: 0.7320 - val_accuracy: 0.6549 - val_loss: 0.8729\n",
      "Epoch 30/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.7100 - loss: 0.7274 - val_accuracy: 0.6594 - val_loss: 0.8692\n",
      "Epoch 31/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.7123 - loss: 0.7226 - val_accuracy: 0.6603 - val_loss: 0.8654\n",
      "Epoch 32/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.7138 - loss: 0.7182 - val_accuracy: 0.6594 - val_loss: 0.8619\n",
      "Epoch 33/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.7170 - loss: 0.7138 - val_accuracy: 0.6621 - val_loss: 0.8587\n",
      "Epoch 34/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.7208 - loss: 0.7095 - val_accuracy: 0.6639 - val_loss: 0.8556\n",
      "Epoch 35/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.7233 - loss: 0.7055 - val_accuracy: 0.6658 - val_loss: 0.8528\n",
      "Epoch 36/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.7256 - loss: 0.7016 - val_accuracy: 0.6658 - val_loss: 0.8502\n",
      "Epoch 37/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.7265 - loss: 0.6977 - val_accuracy: 0.6667 - val_loss: 0.8477\n",
      "Epoch 38/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.7283 - loss: 0.6939 - val_accuracy: 0.6694 - val_loss: 0.8452\n",
      "Epoch 39/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.7300 - loss: 0.6903 - val_accuracy: 0.6730 - val_loss: 0.8429\n",
      "Epoch 40/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.7310 - loss: 0.6868 - val_accuracy: 0.6775 - val_loss: 0.8405\n",
      "Epoch 41/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.7318 - loss: 0.6833 - val_accuracy: 0.6821 - val_loss: 0.8389\n",
      "Epoch 42/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.7332 - loss: 0.6800 - val_accuracy: 0.6821 - val_loss: 0.8371\n",
      "Epoch 43/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.7352 - loss: 0.6767 - val_accuracy: 0.6803 - val_loss: 0.8349\n",
      "Epoch 44/200\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.7354 - loss: 0.6735 - val_accuracy: 0.6803 - val_loss: 0.8332\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import keras\n",
    "\n",
    "activation = \"relu\"\n",
    "    \n",
    "base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(128 ,128, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape = (30, 30, 1)))\n",
    "model.add(layers.Lambda(lambda x: tf.image.resize(x, (128, 128)))) \n",
    "model.add(layers.Lambda(lambda x: tf.image.grayscale_to_rgb(x)))\n",
    "model.add(layers.Lambda(lambda x: preprocess_input(x)))\n",
    "model.add(base_model)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(4, activation = \"softmax\"))\n",
    "\n",
    "model.compile(optimizer = keras.optimizers.Adam(learning_rate= 0.001), \n",
    "              loss = \"categorical_crossentropy\",\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', mode='max', patience=3,  restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=200, validation_data = (X_val, y_val), batch_size=32, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef2f6e8-76c3-443f-b881-335881bf0d28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
